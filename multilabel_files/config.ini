[INPUT]
BERT_VOCAB = /media/data/datasets/biomed/EMBEDDINGS/BERT/multilingual_russian_reviews_finetuned/vocab.txt
;/media/data/datasets/biomed/EMBEDDINGS/BERT/multi_cased_L-12_H-768_A-12/checkpoint/vocab.txt 
;/media/data/datasets/biomed/EMBEDDINGS/BERT/multilingual_russian_reviews_finetuned/vocab.txt
;/media/data/datasets/biomed/EMBEDDINGS/BERT/multi_cased_L-12_H-768_A-12/checkpoint/vocab.txt
BERT_INIT_CHKPNT = otzovik_pretr_rudrec_full_ep20/model.ckpt-84500
;/media/data/datasets/biomed/EMBEDDINGS/BERT/multilingual_russian_reviews_finetuned/bert_model.ckpt 
;pretrained_otzovik_full/model.ckpt-21125 
;/media/data/datasets/biomed/EMBEDDINGS/BERT/rubert_cased_L-12_H-768_A-12_v2/bert_model.ckpt
;psytar_pretrained/model.ckpt-3380 
; /media/data/datasets/biomed/EMBEDDINGS/BERT/multi_cased_L-12_H-768_A-12/checkpoint/bert_model.ckpt
BERT_CONFIG = /media/data/datasets/biomed/EMBEDDINGS/BERT/multilingual_russian_reviews_finetuned/bert_config.json
;/media/data/datasets/biomed/EMBEDDINGS/BERT/multi_cased_L-12_H-768_A-12/checkpoint/bert_config.json
;/media/data/datasets/biomed/EMBEDDINGS/BERT/multilingual_russian_reviews_finetuned/bert_config.json
;/media/data/datasets/biomed/EMBEDDINGS/BERT/multi_cased_L-12_H-768_A-12/checkpoint/bert_config.json
CORPUS_DIR = otzovik_csvs/fold_0/

[PARAMETERS]
# We'll set sequences to be at most 128 tokens long.
MAX_SEQ_LENGTH = 128
# Compute train and warmup steps from batch size
# These hyperparameters are copied from this colab notebook
BATCH_SIZE = 16
LEARNING_RATE = 2e-5
NUM_TRAIN_EPOCHS = 10
# Warmup is a period of time where hte learning rate
# is small and gradually increases--usually helps training.
WARMUP_PROPORTION = 0.1
# Model configs
SAVE_CHECKPOINTS_STEPS = 1000
SAVE_SUMMARY_STEPS = 500

[OUTPUT]
OUTPUT_DIR=res_rudrec_pretr_three_class/ep_20_fold_0/
RESULTS_FILE = multilabel_results.csv
