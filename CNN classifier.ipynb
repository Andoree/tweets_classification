{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "fname = \"/media/data/datasets/biomed/EMBEDDINGS/Otzovik_200_fromScratch.bin\"\n",
    "w2v_model = fasttext.load_model(fname)\n",
    "w2v_model.get_word_vector(fname)\n",
    "embed_dim=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0}\n",
      "(4832, 2)\n",
      "final\n",
      "0.0    4000\n",
      "1.0     832\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path = '/home/tlenusik/9515_tweets_w_labels.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.dropna(subset=[\"final\"], inplace=True)\n",
    "\n",
    "df_0 = df[df['final']==0.0]\n",
    "df_1 = df[df['final']==1.0]\n",
    "\n",
    "df = pd.concat([df_0[:4000], df_1], axis=0, sort=False)\n",
    "\n",
    "df = df[['text','final']]\n",
    "X = df['text'].fillna('').tolist()\n",
    "X = [str(i) for i in X]\n",
    "y = df['final'].tolist()\n",
    "print(set(y))\n",
    "print(df.shape)\n",
    "print(df.groupby(['final']).size())\n",
    "y = [float(i) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split train & test\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Tokenize and transform to integer index\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_test = tokenizer.texts_to_sequences(text_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "maxlen = max(len(x) for x in X_train) # longest text in train set\n",
    "\n",
    "# Add pading to ensure all vectors have same dimensionality\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = w2v_model.get_word_vector(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 65, 200)           3744800   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 61, 128)           128128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 3,874,229\n",
      "Trainable params: 3,874,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3865 samples, validate on 967 samples\n",
      "Epoch 1/10\n",
      "3865/3865 [==============================] - 2s 403us/step - loss: 0.6017 - acc: 0.7884 - val_loss: 0.5548 - val_acc: 0.8221\n",
      "Epoch 2/10\n",
      "3865/3865 [==============================] - 1s 199us/step - loss: 0.3468 - acc: 0.8476 - val_loss: 0.4149 - val_acc: 0.8263\n",
      "Epoch 3/10\n",
      "3865/3865 [==============================] - 1s 196us/step - loss: 0.2258 - acc: 0.9063 - val_loss: 0.4390 - val_acc: 0.8294\n",
      "Epoch 4/10\n",
      "3865/3865 [==============================] - 1s 191us/step - loss: 0.1454 - acc: 0.9472 - val_loss: 0.4689 - val_acc: 0.8190\n",
      "Epoch 5/10\n",
      "3865/3865 [==============================] - 1s 195us/step - loss: 0.0773 - acc: 0.9796 - val_loss: 0.5188 - val_acc: 0.8294\n",
      "Epoch 6/10\n",
      "3865/3865 [==============================] - 1s 196us/step - loss: 0.0524 - acc: 0.9884 - val_loss: 0.5746 - val_acc: 0.8232\n",
      "Epoch 7/10\n",
      "3865/3865 [==============================] - 1s 194us/step - loss: 0.0271 - acc: 0.9959 - val_loss: 0.6372 - val_acc: 0.8335\n",
      "Epoch 8/10\n",
      "3865/3865 [==============================] - 1s 187us/step - loss: 0.0148 - acc: 0.9984 - val_loss: 0.6499 - val_acc: 0.8366\n",
      "Epoch 9/10\n",
      "3865/3865 [==============================] - 1s 186us/step - loss: 0.0136 - acc: 0.9987 - val_loss: 0.6418 - val_acc: 0.8314\n",
      "Epoch 10/10\n",
      "3865/3865 [==============================] - 1s 194us/step - loss: 0.0120 - acc: 0.9990 - val_loss: 0.7025 - val_acc: 0.8345\n",
      "3865/3865 [==============================] - 0s 35us/step\n",
      "Training Accuracy: 0.9992\n",
      "Testing Accuracy:  0.8345\n"
     ]
    }
   ],
   "source": [
    "# Define CNN architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "#model.add(layers.Embedding(vocab_size, embed_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=True)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = model.predict_classes(X_test).flatten().tolist()\n",
    "#print('Prediction: ', y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_sample).tolist()\n",
    "p1w, r1w, f1w, _ = metrics.precision_recall_fscore_support(y_test, y_sample, average='weighted')\n",
    "p1m, r1m, f1m, _ = metrics.precision_recall_fscore_support(y_test, y_sample, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[755,  36],\n",
       "       [124,  52]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.95      0.90       791\n",
      "         1.0       0.59      0.30      0.39       176\n",
      "\n",
      "    accuracy                           0.83       967\n",
      "   macro avg       0.72      0.62      0.65       967\n",
      "weighted avg       0.81      0.83      0.81       967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.79      0.81       402\n",
      "         1.0       0.55      0.64      0.60       165\n",
      "\n",
      "    accuracy                           0.75       567\n",
      "   macro avg       0.70      0.72      0.71       567\n",
      "weighted avg       0.76      0.75      0.75       567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_docs(fname):\n",
    "    texts = []\n",
    "    with open(fname, \"r\", encoding='utf-8') as fin:\n",
    "        lines = [line for line in fin]\n",
    "    for line in lines:\n",
    "        try:\n",
    "            doc = json.loads(line)\n",
    "            texts.append(doc['description'])\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            pass\n",
    "    return texts\n",
    "FILE = \"/media/data/datasets/biomed/otzovik/corpora/otzovik/all_reviews_texts.txt\"\n",
    "texts = load_docs(FILE)\n",
    "with open(\"/media/data/datasets/biomed/otzovik/corpora/otzovik/data_temp.txt\", \"w+\", encoding='utf-8') as fout:\n",
    "    for line in texts:\n",
    "        fout.write(\"{}\\n\".format(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "d = 200\n",
    "model = fasttext.train_unsupervised('/media/data/datasets/biomed/otzovik/corpora/otzovik/data_temp.txt', model='cbow', dim=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"/media/data/datasets/biomed/EMBEDDINGS/Otzovik_{}_fromScratch.bin\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\r\n",
      "MedAll_300_fromScratch.bin\r\n",
      "Otzovik_200_fromScratch.bin\r\n",
      "PubMedVec.bin\r\n",
      "idf_genes_phc.txt\r\n",
      "medline_abstract_word2vec_2012.bin\r\n",
      "medline_abstract_word2vec_2012.txt\r\n",
      "medline_abstract_word2vec_2012_mapped.txt\r\n",
      "medline_abstract_word2vec_2013\r\n",
      "medline_abstract_word2vec_2013.bin\r\n",
      "medline_abstract_word2vec_2013.syn0.npy\r\n",
      "medline_abstract_word2vec_2013.syn1neg.npy\r\n",
      "medline_abstract_word2vec_2013.txt\r\n",
      "medline_abstract_word2vec_2013_mapped.txt\r\n",
      "medline_abstract_word2vec_2013_modified.bin\r\n",
      "medline_abstract_word2vec_2014\r\n",
      "medline_abstract_word2vec_2014.bin\r\n",
      "medline_abstract_word2vec_2014.syn0.npy\r\n",
      "medline_abstract_word2vec_2014.syn1neg.npy\r\n",
      "medline_abstract_word2vec_2014.txt\r\n",
      "medline_abstract_word2vec_2014_mapped.txt\r\n",
      "medline_abstract_word2vec_2014_modified.bin\r\n",
      "mesh_single_terms.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /media/data/datasets/biomed/EMBEDDINGS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
